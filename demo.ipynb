{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062045fe-44e3-45a2-b35e-498d43fc091f",
   "metadata": {},
   "source": [
    "# ESGF Virtual Aggregation - Demo\n",
    "\n",
    "Remote data access to Virtual Analysis Ready Data (Virtual ARD) for climate datasets of the [ESGF](https://esgf.llnl.gov/).\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "  * [Loading the inventory](#loading-the-inventory)\n",
    "  * [Loading a dataset](#loading-a-dataset)\n",
    "  * [Query attributes](#query-attributes)\n",
    "  * [Performing data analysis](#performing-data-analysis)\n",
    "* [Similarities with other approaches](#similarities-with-other-approaches)\n",
    "  * [Pangeo CMIP6](#pangeo-cmip6)\n",
    "  * [Cloud native repositories](#cloud-native-repositories)\n",
    "  * [ROOCS](#roocs) \n",
    "* [Discussion and FAQ](#discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6fa062-8f40-4c76-99e2-1c2b92dd3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import xarray\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "dask.config.set(scheduler=\"processes\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 12, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588105d0-4a56-4635-b748-f1700efb1aee",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "## Loading the inventory <a class=\"anchor\" id=\"loading-the-inventory\"></a>\n",
    "\n",
    "Currently, the invetory is a CSV file which can be queried for locating the desired dataset. You may also inspect the [THREDDS catalog](https://hub.ipcc.ifca.es/thredds/catalog/esgeva/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc28a0-72a9-4cdf-8394-7089b039a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://hub.ipcc.ifca.es/thredds/fileServer/public/inventory.csv\")\n",
    "\n",
    "# Find a dataset\n",
    "subset = df.query(\"product == 'ScenarioMIP' & model == 'CNRM-CM6-1' & experiment == 'ssp245' & table == 'day' & variable == 'tas' & version == 'v20190410'\")\n",
    "\n",
    "# See available data nodes\n",
    "subset[[\"location\", \"replica\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca201b5-c44e-46b5-8b30-b08ebdb9b810",
   "metadata": {},
   "source": [
    "Once you have located the desired URL for your dataset, save its URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a33a77-abb8-466c-8f9b-ede009f3c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a data node and get the url\n",
    "url = subset.query(\"replica.isnull()\")[\"location\"].iloc[0]\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041a88c-7565-4a3f-872d-dd09001acdfa",
   "metadata": {},
   "source": [
    "## Loading a dataset <a class=\"anchor\" id=\"loadingg-a-dataset\"></a>\n",
    "\n",
    "Now that you have an URL, try to load the dataset using [xarray](https://docs.xarray.dev/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260df95-4468-4715-9239-74915719c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a demo URL for demonstration (see https://hub.ipcc.ifca.es/thredds/catalog/esgeva/demo/catalog.html)\n",
    "url = \"https://hub.ipcc.ifca.es/thredds/dodsC/esgeva/demo/CMIP6_ScenarioMIP_MIROC_MIROC-ES2L_ssp245_3hr_tas_gn_v20210107_esgf-data02.diasjp.net.ncml\"\n",
    "\n",
    "# Load the URL\n",
    "ds = xarray.open_dataset(url)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330266e-2da0-45bc-b026-61d55eb781bd",
   "metadata": {},
   "source": [
    "### Query attributes <a class=\"anchor\" id=\"query-attributes\"></a>\n",
    "\n",
    "The ESGF Virtual Aggregation includes metadata of interest for the users. Let's inspect some attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406d73e-c648-4059-b7d5-43959b0263e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the size of the dataset on the server side (sum of file sizes on the server\n",
    "ds.attrs[\"size_human\"] # Note that this might be different from the size in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e2736-e18a-42e8-982b-b2f52e396866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the variant_label (ensemble) coordinate\n",
    "ds[\"variant_label\"][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726c8a8-eb17-4e2f-8029-e1c85661b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the PID handles of the virtual dataset\n",
    "ds[\"tracking_id\"][:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939aceef-af62-42ad-b1ea-78f4dc37f0a3",
   "metadata": {},
   "source": [
    "## Performing data analysis <a class=\"anchor\" id=\"performing-data-analysis\"></a>\n",
    "\n",
    "Now that we have loaded the dataset, we can perform data analysis on it. First, let's chunk the dataset to allow scalability through [Dask]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505c365-94fc-49c0-b007-494ab0c5acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc = ds.chunk({\"variant_label\":1, \"time\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bd704-bdcb-431c-8dd3-e989ce4eb614",
   "metadata": {},
   "source": [
    "Now, inspect the structure of the data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b480075-e74e-445a-9fec-0a6f7a111f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc[\"tas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d357136-2e8f-4842-9a80-74542cbe92f0",
   "metadata": {},
   "source": [
    "We are ready to perform data analysis of an small subset of the array and plot the results. Note that only the required data is transferred through the networking, involving no file downloads at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39c01e-19d3-4c10-a5fc-98f86ce27b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [\n",
    "    np.datetime64('2100-01-01'),\n",
    "    np.datetime64('2100-03-15'),\n",
    "    np.datetime64('2100-06-15'),\n",
    "    np.datetime64('2100-09-15'),\n",
    "    np.datetime64('2100-12-31')]\n",
    "da = dsc[\"tas\"].sel(variant_label=[b\"r10i1p1f2\", b\"r11i1p1f2\"], time=slice(\"21000101\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d82077-b023-4005-ba5d-17e05742b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223239-8c08-4285-92a2-e90e2ada835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time results = da.groupby_bins(\"time\", bins).mean().compute(nworkers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d65371-4549-4f51-9db5-9871cfb91550",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.assign_coords({\"time_bins\": [\"JFM\", \"MAJ\", \"JAS\", \"OND\"]}).plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"time_bins\", row=\"variant_label\",\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kwargs={\"shrink\": .6},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa37461-e70c-45e4-83fa-8e5ca06f3c87",
   "metadata": {},
   "source": [
    "# Similarities with other approaches <a class=\"anchor\" id=\"similarities-with-other-approaches\"/>\n",
    "\n",
    "Currently there is a strong emphasis in providing capabilities for easy data analysis of climate data, due to productivity that can be achieved. The ESGF Virtual Aggregation is one of these attemps and now we discuss how it compares to different approaches from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9f527-b734-4358-b024-9564dd58c2b3",
   "metadata": {},
   "source": [
    "## Pangeo CMIP6 <a class=\"anchor\" id=\"pangeo-cmip6\"/>\n",
    "\n",
    "People from Pangeo are setting up cloud repositories for CMIP6 ARCO (Analysis Ready Cloud Optimized) datasets from ESGF. While the performance of these repositories is much superior to the data nodes of the ESGF, the cost of implementing these repositories is huge, due to the cost of duplicating the data. The ESGF Virtual Aggregation is less performant than cloud repositories but it is also much cheaper to build.\n",
    "\n",
    "The ESGF Virtual Aggregation should be seen as an intermediate approach between the current state of the ESGF federation and more ground-breaking solutions like cloud repositories. Its main purpose is to show the compute capabilites that can be reached building on top of the existing federation.\n",
    "\n",
    "[Here](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/basic_search_and_load.html) you can find the basic example for data search and load from the Pangeo repository. We try to reproduce this here using the ESGF Virtual Aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff31e4-9579-4fc2-80d0-468871e508b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.query(\"product == 'CMIP' & model == 'CESM2' & experiment == 'historical' & table == 'Amon' & variable == 'tas' & version == 'v20190311'\")\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82d4d-e180-4efd-b9d5-6f5a5eb1d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = subset.iloc[0][\"location\"]\n",
    "ds = xarray.open_dataset(url).chunk({\"time\": 100})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b2fcb-aad2-4990-af13-c60839e94ed4",
   "metadata": {},
   "source": [
    "Plot a map from a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f19f71-e5d6-498f-ae69-ce9944dc2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tas.sel(time='1950-01', variant_label=b\"r9i1p1f1\").squeeze().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ad183-3a9a-43d8-84fa-ce4e0bd7533c",
   "metadata": {},
   "source": [
    "Create a timeseries of global-average surface air temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4979e4-0e78-4c74-9f9f-4ebce5d62ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "areacella = xarray.open_dataset(\"http://esgf-data04.diasjp.net/thredds/dodsC/esg_dataroot/CMIP6/CMIP/NCAR/CESM2/historical/r9i1p1f1/fx/areacella/gn/v20190311/areacella_fx_CESM2_historical_r9i1p1f1_gn.nc\")\n",
    "areacella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42481b-6151-4158-94e3-93e501403117",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_area = areacella[\"areacella\"].sum(dim=[\"lon\", \"lat\"])\n",
    "ta_timeseries = (ds[\"tas\"].sel(variant_label=b\"r9i1p1f1\") * areacella[\"areacella\"]).sum(dim=[\"lon\", \"lat\"]) / total_area\n",
    "ta_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33768226-b758-4d14-92d8-6b46a44c6c92",
   "metadata": {},
   "source": [
    "By default the data are loaded lazily, as Dask arrays. Here we trigger computation explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcdc19f-d6bb-4282-96fc-fa261f257a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ta_timeseries.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d911fe5-5a0b-49dc-82a0-75111bc31326",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_timeseries.plot(label=\"monthly\")\n",
    "ta_timeseries.rolling(time=12).mean().plot(label=\"12 month rolling mean\")\n",
    "plt.legend()\n",
    "plt.title(\"Global Mean Surface Air Temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ebddd-d01c-491d-9fb9-c6ad3574e115",
   "metadata": {},
   "source": [
    "## Cloud native repositories <a class=\"anchor\" id=\"cloud-native-repositories\"/>\n",
    "\n",
    "This work is a bridge between the current state of the federation and more elaborated [ETL](https://es.wikipedia.org/wiki/Extract,_transform_and_load) attemps such as [Google CMIP6](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/basic_search_and_load.html) from [Pangeo](https://pangeo.io/). The later is a much more expensive workflow that requires duplication of the datasets into a cloud provider, which in advantage offers much more scalable data service compared to the \"best effort\" basis of the ESGF data nodes. Also, the ESGF Virtual Aggregation offers a higher level Analysis Ready Dataset by adding the `ensemble` dimension to the variables (see the [Virtual Aggregation file](https://hub.ipcc.ifca.es/thredds/fileServer/esgeva/ensemble/CMIP6/ScenarioMIP/day/CMIP6_ScenarioMIP_CNRM-CERFACS_CNRM-CM6-1_ssp245_day_gr_v20190410/replicas/aims3.llnl.gov/CMIP6_ScenarioMIP_CNRM-CERFACS_CNRM-CM6-1_ssp245_day_tas_gr_v20190410_aims3.llnl.gov.ncml)).\n",
    "\n",
    "The ESGF Virtual Aggregation could act as an intermediate between the ESGF and cloud native repositories. Because ESGF Virtual Aggregation is much cheaper to run (since it only reads metadata from the ESGF distributed index), cloud repositories using cloud optimized formats (Zarr) can be created much easier. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645fa66-c229-4eef-b4c7-c260a5534f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a dataset from the ESGF Virtual Aggregation\n",
    "ds = xarray.open_dataset(\"https://hub.ipcc.ifca.es/thredds/dodsC/esgeva/demo/CMIP6_ScenarioMIP_MIROC_MIROC-ES2L_ssp245_3hr_tas_gn_v20210107_esgf-data02.diasjp.net.ncml\").chunk({\"variant_label\": 1, \"time\": 736})\n",
    "\n",
    "# Extract a subset from the ESGF Virtual Aggregation\n",
    "subset = ds.sel(variant_label=[b\"r10i1p1f2\", b\"r11i1p1f2\", b\"r12i1p1f2\"], time=slice(\"21000601\", \"21000831\"))\n",
    "subset[\"tas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedc355-bf56-4c5d-8109-06e55fed879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf test.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050688a-6ed1-4a10-bd51-2c6453cef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to a cloud native repository\n",
    "subset.to_zarr(\"test.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb920-2a09-4af6-8d95-92838865c939",
   "metadata": {},
   "source": [
    "## ROOCS <a class=\"anchor\" id=\"roocs\"/>\n",
    "\n",
    "[ROOCS](https://github.com/roocs) is a project to develop data services in support of the Copernicus Climate Change Service (C3S). ROOCS is providing a collection of tools to provide data-aware processing services of climate projections from CMIP6, CMIP5 and CORDEX.\n",
    "\n",
    "The main focus of the service is to reduce the volumes of data transferred by providing data-reduction processes that can be invoked directly from the C3S Climate Data Store (CDS).\n",
    "\n",
    "[Here](https://nbviewer.org/github/roocs/rooki/blob/master/notebooks/demo/demo-rooki-concat-with-average-cmip6-decadal.ipynb) you can find an example of a ROOCS workflow. We try to reproduce the workflow here using the ESGF Virtual Aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6caba-171d-4f59-a09f-0bf680b84358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMIP6.DCPP.MOHC.HadGEM3-GC31-MM.dcppA-hindcast.s1995-r1i1p1f2.Amon.tas.gn.v20200417\n",
    "subset = df.query(\"product == 'DCPP' & model == 'HadGEM3-GC31-MM' & experiment == 'dcppA-hindcast' & subexperiment == 's1995' & table == 'Amon' & variable == 'tas' & version == 'v20200417'\")\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08522372-6a25-4119-82aa-a12b29b1ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = subset.query(\"replica == 'esgf.nci.org.au'\").iloc[0][\"location\"]\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72338902-1b7c-47eb-ba04-74d5b39191fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset(url).chunk({\"variant_label\": 1, \"time\": 75})\n",
    "ds[\"tas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57605494-7182-48b2-82b3-d025575ac6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time a = ds[\"tas\"].mean([\"lat\", \"lon\"]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885df53e-a930-4728-843f-893c0d04bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.plot.line(x=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c81551-f42c-4e17-bddc-4f31061ce853",
   "metadata": {},
   "source": [
    "## ENSO from ESGF\n",
    "\n",
    "            activity_id=\"CMIP\",\n",
    "            experiment_id=\"historical\",\n",
    "            institution_id=institution_id,\n",
    "            variable_id=[\"areacello\"],\n",
    "            member_id='r11i1p1f1',\n",
    "\n",
    "                        activity_id=\"CMIP\",\n",
    "            experiment_id=\"historical\",\n",
    "            institution_id=institution_id,\n",
    "            variable_id=[\"tos\"],\n",
    "            member_id='r11i1p1f1',\n",
    "            table_id=\"Omon\",\n",
    "\n",
    "https://github.com/esgf2-us/esgf-cookbook/blob/main/notebooks/enso-globus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a494e80-a061-4306-960f-4ccbd001f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cf_xarray\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee315f2e-12ae-4485-9812-3e3b3cb51590",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.query(\"product == 'CMIP' & experiment == 'historical' & variable == 'areacello' & institution == 'NCAR' & replica.isnull()\").groupby(\"model\").last()\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66252b-7cd7-4b4f-ba18-c9a71b56739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.concat([\n",
    "    df.query(\"model != 'CESM2-WACCM' & product == 'CMIP' & experiment == 'historical' & variable == 'areacello' & institution == 'NCAR' & replica.isnull()\").groupby(\"model\").last(),\n",
    "    df.query(\"model != 'CESM2-WACCM' & product == 'CMIP' & experiment == 'historical' & variable == 'tos' & table == 'Omon' & institution == 'NCAR' & replica.isnull()\").groupby(\"model\").last(),\n",
    "])\n",
    "\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0557239-801b-4d11-9151-c6fe7b4f34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "areacello = subset[subset[\"variable\"] == \"areacello\"]\n",
    "area_ds = xarray.concat([xarray.open_dataset(url)[\"areacello\"].isel(variant_label=0) for url in areacello[\"location\"]], areacello.index)\n",
    "area_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258024a5-3aa4-47e3-aa58-11ac9535a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "tos = subset[subset[\"variable\"] == \"tos\"]\n",
    "tos_ds = xarray.concat([xarray.open_dataset(url)[\"tos\"].isel(variant_label=0) for url in tos[\"location\"]], tos.index)\n",
    "tos_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46649f-7f2d-4353-8376-db30591c3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.merge([area_ds, tos_ds]).drop_vars('variant_label')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053ec49-75ee-4c8a-9397-d5682ba212e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_enso(ds):\n",
    "\n",
    "    # Subset the El Nino 3.4 index region\n",
    "    dso = ds.where(\n",
    "        (ds.cf[\"latitude\"] < 5) &\n",
    "        (ds.cf[\"latitude\"] > -5) &\n",
    "        (ds.cf[\"longitude\"] > 190) &\n",
    "        (ds.cf[\"longitude\"] < 240),\n",
    "        drop=True)\n",
    "\n",
    "    # Calculate the monthly means\n",
    "    gb = dso.tos.groupby('time.month')\n",
    "\n",
    "    # Subtract the monthly averages, returning the anomalies\n",
    "    tos_nino34_anom = gb - gb.mean(dim='time')\n",
    "\n",
    "    # Determine the non-time dimensions and average using these\n",
    "    non_time_dims = set(tos_nino34_anom.dims)\n",
    "    non_time_dims.remove(ds.tos.cf[\"T\"].name)\n",
    "    weighted_average = tos_nino34_anom.weighted(ds[\"areacello\"]).mean(dim=list(non_time_dims))\n",
    "\n",
    "    # Calculate the rolling average\n",
    "    rolling_average = weighted_average.rolling(time=5, center=True).mean()\n",
    "    std_dev = weighted_average.std()\n",
    "    return rolling_average / std_dev\n",
    "\n",
    "def add_enso_thresholds(da, threshold=0.4):\n",
    "\n",
    "    # Conver the xr.DataArray into an xr.Dataset\n",
    "    ds = da.to_dataset()\n",
    "\n",
    "    # Cleanup the time and use the thresholds\n",
    "    try:\n",
    "        ds[\"time\"]= ds.indexes[\"time\"].to_datetimeindex()\n",
    "    except:\n",
    "        pass\n",
    "    ds[\"tos_gt_04\"] = (\"time\", ds.tos.where(ds.tos >= threshold, threshold).data)\n",
    "    ds[\"tos_lt_04\"] = (\"time\", ds.tos.where(ds.tos <= -threshold, -threshold).data)\n",
    "\n",
    "    # Add fields for the thresholds\n",
    "    ds[\"el_nino_threshold\"] = (\"time\", np.zeros_like(ds.tos) + threshold)\n",
    "    ds[\"la_nina_threshold\"] = (\"time\", np.zeros_like(ds.tos) - threshold)\n",
    "\n",
    "    return ds\n",
    "\n",
    "enso_index = add_enso_thresholds(calculate_enso(ds).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988deaff-5676-4e61-8cf7-dcd9ead7d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e2371-a755-4948-b1c0-f9c184f81d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "plt.axhline(y=0.4, color='black', linestyle='-', linewidth=.5)\n",
    "plt.axhline(y=-0.4, color='black', linestyle='-', linewidth=.5)\n",
    "\n",
    "plt.fill_between(enso_index[\"time\"], enso_index[\"tos_gt_04\"].data, 0.4)\n",
    "plt.fill_between(enso_index[\"time\"], enso_index[\"tos_lt_04\"].data, -0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2af2b6-eae0-4629-a8a2-3ddbc44b26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data\n",
    "# ds.to_netcdf(\"enso.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4749b1d-4286-4522-9adf-1cad1da21ca4",
   "metadata": {},
   "source": [
    "# Discussion <a class=\"anchor\" id=\"discussion\"></a>\n",
    "\n",
    "## How it works?\n",
    "\n",
    "**ALL** ESGF file metada is queried into a local database and from this database, small metadata [NcML]() files are created. The NcML files allow to merge multiple netCDF files into a single dataset. netCDF files from the federation are referenced by their OPeNDAP endpoint, thus no data movement is needed from the original data nodes.\n",
    "\n",
    "```xml\n",
    "<aggregation type=\"joinNew\" dimName=\"variant_label\">\n",
    "    <variableAgg name=\"tas\" />\n",
    "        <netcdf coordValue=\"r2i1p1f2\">\n",
    "            <aggregation type=\"joinExisting\" dimName=\"time\">\n",
    "                <netcdf location=\"http://aims3.llnl.gov/thredds/dodsC/css03_data/CMIP6/ScenarioMIP/CNRM-CERFACS/CNRM-CM6-1/ssp245/r2i1p1f2/day/tas/gr/v20190410/tas_day_CNRM-CM6-1_ssp245_r2i1p1f2_gr_20150101-21001231.nc\" ncoords=\"31411\"/>\n",
    "            </aggregation>\n",
    "        </netcdf>\n",
    "        <netcdf coordValue=\"r3i1p1f2\">\n",
    "            <aggregation type=\"joinExisting\" dimName=\"time\">\n",
    "                <netcdf location=\"http://aims3.llnl.gov/thredds/dodsC/css03_data/CMIP6/ScenarioMIP/CNRM-CERFACS/CNRM-CM6-1/ssp245/r3i1p1f2/day/tas/gr/v20190410/tas_day_CNRM-CM6-1_ssp245_r3i1p1f2_gr_20150101-21001231.nc\" ncoords=\"31411\"/>\n",
    "            </aggregation>\n",
    "        </netcdf>\n",
    "</aggregation>\n",
    "```\n",
    "\n",
    "## Some ESGF Virtual Aggregation datasets, specially from the ensemble aggregation, take forever to open, why?\n",
    "\n",
    "One of the main drawbacks of the current implementation of the ESGF Virtual Aggregation is that it takes a lot of time to open virtual datasets that reference lots (more than 100) files. Because no information is available in the ESGF index about the time coordinate, there is no way to tell the virtual dataset the size of the time dimension. Thus, the virtual dataset tries to obtain this from the files but this involves a huge amount of HTTP requests because it needs to query all the files in the virtual dataset. If only information about the size of the time dimension and the values from the time coordinate were available in the ESGF index, this performance issue would be solved.\n",
    "\n",
    "## How does OPeNDAP compare to Zarr for remote data analysis?\n",
    "\n",
    "In summary, Zarr is more efficient for remote data analysis. When requesting a piece of data from a Zarr store, the chunks travel compressed to the client. On the other hand, OPeNDAP servers uncompress the chunks from netCDF files on the server and send the data uncompressed to the client, making the transfer more inefficient.\n",
    "\n",
    "## How does OPeNDAP compare to server side computing (eg: ROOCS)?\n",
    "\n",
    "Both OPeNDAP and cloud native data stores aim to provide remote data analysis capabilities. Because the data is required to travel through the Internet, I/O performance is far from ideal. Server side computing such as ROOCS receive the compute request on the server and compute the result near to the data with much faster I/O. In general, remote data analysis serves for exploratory tasks and if fast I/O is needed, compute resources should be bought/requested in the infrastructure where the data resides.\n",
    "\n",
    "## How does NcML compare to Kerchunk?\n",
    "\n",
    "While Kerchunk focuses on creating a metada file to locate chunks of data in different formats, it can be use to create virtual datasets too (see [this](https://fsspec.github.io/kerchunk/tutorial.html#combine-multiple-kerchunked-datasets-into-a-single-logical-aggregate-dataset)). NcML serves a similar purpose but it works with CDM compatible datasets (see [netCDF-java CDM](https://docs.unidata.ucar.edu/netcdf-java/current/userguide/common_data_model_overview.html)).\n",
    "\n",
    "## Does OPeNDAP scale?\n",
    "\n",
    "OPeNDAP is provided by the [THREDDS Data Server]() in ESGF data nodes. The TDS is a java web application and its ability to scale is the same as any other java web application. If deployed in a single application server, scalibility is limited to the single machine. If a kubernetes clustes is available to spawm multiple application servers, then it can scale and provide more performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ccf2-cf83-4079-a772-00ad3ff87eb3",
   "metadata": {},
   "source": [
    "## How are netCDF files distributed between NcMLs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5cb7f-682c-4792-9673-282ffe00a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 4))\n",
    "\n",
    "    with zipfile.ZipFile(\"ncml_dist.csv.zip\", \"r\") as zip_file:\n",
    "        df = pd.read_csv(zip_file.namelist()[0], names=[\"name\", \"count\"], skiprows=1)\n",
    "\n",
    "    bins = [0, 1, 5, 10, 100, 1000, df[\"count\"].max()]\n",
    "    cuts = pd.cut(df[\"count\"], bins=bins).value_counts(sort=False)\n",
    "    cuts.plot.bar(rot=0, color=\"b\", ax=ax)\n",
    "    ax.set_xticklabels([\"<={}\\n{}%\".format(c.right, round(v/len(df)*100,2)) for c,v in zip(cuts.index.categories, cuts.values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f74549-46a8-4de1-8998-7113569ecb2c",
   "metadata": {},
   "source": [
    "## Why is the time variable a problem in ESGF?\n",
    "\n",
    "In HDF5, chunks are stored in a B-Tree and very small values make the B-Tree unnecessary large. NetCDF files in ESGF usually use a chunk size of (1,) for the time coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640969ec-df0d-452a-af56-a09745c8c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [256, 512, 1024, 2048, 4096, 8192]\n",
    "results = []\n",
    "\n",
    "def test(dset):\n",
    "    return dset[...].mean()\n",
    "\n",
    "for size in sizes:\n",
    "    with netCDF4.Dataset(\"test.nc\", \"w\") as f:\n",
    "        f.createDimension(\"time\", size)\n",
    "        f.createVariable(\"time1\", \"f8\", (\"time\",))\n",
    "        f.createVariable(\"time2\", \"f8\", (\"time\",), chunksizes=(1,))\n",
    "\n",
    "        f[\"time1\"][...] = list(range(size))\n",
    "        f[\"time2\"][...] = list(range(size))\n",
    "\n",
    "    with netCDF4.Dataset(\"test.nc\", \"r\") as f:\n",
    "        results.append(timeit.timeit(lambda : test(f[\"time1\"]), number=100))\n",
    "        results.append(timeit.timeit(lambda : test(f[\"time2\"]), number=100))\n",
    "\n",
    "plt.plot(sizes, results[0::2], '--bo', label=\"Contiguous\")\n",
    "plt.plot(sizes, results[1::2], '--ro', label=\"Chunksizes = 1\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size of the time dimension\")\n",
    "plt.ylabel(\"Time to load the time values\")\n",
    "plt.title(\"Load time for sizes [256, 512, 1024, 2048, 4096, 8192]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b34cbc-e404-4807-bf6c-8273425a9a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
